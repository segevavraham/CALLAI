// Twilio Media Streams <-> n8n WebSocket Bridge - OPTIMIZED FOR REAL-TIME
const express = require('express');
const WebSocket = require('ws');
const axios = require('axios');
const { spawn } = require('child_process');
const fs = require('fs').promises;
const path = require('path');

const app = express();
const PORT = process.env.PORT || 3000;

// âš™ï¸ ×”×’×“×¨×•×ª - OPTIMIZED FOR REAL-TIME CONVERSATION
const N8N_WEBHOOK_URL = process.env.N8N_WEBHOOK_URL || 'https://segevavraham.app.n8n.cloud/webhook/twilio-process-audio';
const SILENCE_TIMEOUT = 800; // 800ms - fast response while avoiding word cuts âš¡
const MIN_AUDIO_CHUNKS = 12; // Minimum chunks - works for short utterances too ğŸ¤
const CHUNK_SIZE = 160; // Twilio standard
const MAX_IDLE_TIME = 30000; // 30 seconds of silence before timeout warning
const CONVERSATION_TIMEOUT = 300000; // 5 minutes total conversation limit
const MAX_HISTORY_MESSAGES = 50; // Maximum messages to keep in history (prevent memory issues)

// ××—×¡×•×Ÿ ×–×× ×™ ×©×œ ×—×™×‘×•×¨×™ WebSocket ×¤×¢×™×œ×™×
const activeCalls = new Map();

// ğŸ¥ Health check endpoint
app.get('/health', (req, res) => {
  res.json({
    status: 'healthy',
    uptime: process.uptime(),
    activeCalls: activeCalls.size,
    timestamp: new Date().toISOString(),
    config: {
      silenceTimeout: SILENCE_TIMEOUT,
      minAudioChunks: MIN_AUDIO_CHUNKS,
      maxIdleTime: MAX_IDLE_TIME,
      maxHistoryMessages: MAX_HISTORY_MESSAGES
    }
  });
});

// ğŸ“Š Stats endpoint
app.get('/stats', (req, res) => {
  const stats = {
    activeCalls: activeCalls.size,
    calls: []
  };

  activeCalls.forEach((callData, callSid) => {
    stats.calls.push({
      callSid,
      duration: Math.round((Date.now() - callData.startTime) / 1000),
      turns: callData.turnCount,
      historySize: callData.conversationHistory.length,
      isProcessing: callData.isProcessing,
      isSpeaking: callData.currentAudioPlaying
    });
  });

  res.json(stats);
});

// Express endpoint ×œ-TwiML ×©×œ Twilio
app.get('/voice', (req, res) => {
  const wsUrl = `wss://${req.get('host')}/media-stream`;

  const twiml = `<?xml version="1.0" encoding="UTF-8"?>
<Response>
    <Connect>
        <Stream url="${wsUrl}" />
    </Connect>
</Response>`;

  res.type('text/xml');
  res.send(twiml);
});

// ×”×¤×¢×œ HTTP server
const server = app.listen(PORT, () => {
  console.log(`ğŸš€ Server running on port ${PORT}`);
  console.log(`ğŸ¯ Twilio-n8n WebSocket Bridge - ENTERPRISE QUALITY`);
  console.log(`\nğŸ“‹ Configuration:`);
  console.log(`   âš¡ Silence timeout: ${SILENCE_TIMEOUT}ms`);
  console.log(`   ğŸ¤ Min audio chunks: ${MIN_AUDIO_CHUNKS}`);
  console.log(`   â±ï¸  Max idle time: ${MAX_IDLE_TIME / 1000}s`);
  console.log(`   ğŸ”„ Conversation timeout: ${CONVERSATION_TIMEOUT / 1000}s`);
  console.log(`\nâœ… Features:`);
  console.log(`   ğŸ“š Full conversation history tracking`);
  console.log(`   ğŸš¨ Automatic error recovery`);
  console.log(`   â±ï¸  Intelligent timeout management`);
  console.log(`   ğŸ“Š Detailed call analytics\n`);
});

// ×”×¤×¢×œ WebSocket server
const wss = new WebSocket.Server({ server, path: '/media-stream' });

wss.on('connection', (ws) => {
  console.log('ğŸ“ New Twilio call connected');

  let callSid = null;
  let streamSid = null;
  let silenceTimeout = null;
  let welcomeSent = false;

  ws.on('message', async (message) => {
    try {
      const msg = JSON.parse(message);

      switch (msg.event) {
        case 'start':
          callSid = msg.start.callSid;
          streamSid = msg.start.streamSid;
          console.log(`ğŸ“ Call started: ${callSid}`);
          activeCalls.set(callSid, {
            ws,
            streamSid,
            audioBuffer: [],
            isProcessing: false,
            currentAudioPlaying: false,  // âœ… Track AI speaking state
            conversationHistory: [],     // ğŸ“š Full conversation context
            startTime: Date.now(),       // â±ï¸  Track call duration
            lastActivityTime: Date.now(), // ğŸ‘‚ Track user activity
            turnCount: 0,                // ğŸ”¢ Track conversation turns
            idleWarningsSent: 0          // âš ï¸  Track timeout warnings
          });
          
          // â±ï¸  ×”×¤×¢×œ × ×™×”×•×œ timeout
          setupIdleTimeout(callSid);

          // ×©×œ×— ×”×•×“×¢×ª ×¤×ª×™×—×” ×‘×¢×‘×¨×™×ª ××™×“!
          if (!welcomeSent) {
            welcomeSent = true;
            console.log('ğŸ‘‹ Sending welcome message in 300ms...');
            setTimeout(() => {
              sendWelcomeMessage(callSid, streamSid, ws);
            }, 300);
          }
          break;

        case 'media':
          // ×× AI ××“×‘×¨ - ignore user audio (prevent interruption feedback loop)
          const callData = activeCalls.get(callSid);
          if (!callData) {
            console.warn(`âš ï¸  No call data for ${callSid}`);
            break;
          }

          if (callData.currentAudioPlaying) {
            // AI is speaking, ignore user input to prevent feedback
            // console.log(`ğŸ”‡ Ignoring user input - AI speaking`);
            break;
          }

          if (callData.isProcessing) {
            // Already processing previous input, buffer this
            // console.log(`â¸ï¸  Buffering while processing`);
          }

          // âœ… Use callData.audioBuffer instead of local variable
          callData.audioBuffer.push(msg.media.payload);

          // âš¡ Real-time VAD
          clearTimeout(silenceTimeout);
          silenceTimeout = setTimeout(async () => {
            const currentCallData = activeCalls.get(callSid);
            if (!currentCallData) return;

            // Only process if we have enough audio AND not currently processing
            if (currentCallData.audioBuffer.length >= MIN_AUDIO_CHUNKS && !currentCallData.isProcessing) {
              currentCallData.isProcessing = true;
              const chunksToProcess = [...currentCallData.audioBuffer];
              currentCallData.audioBuffer = []; // âœ… Clear buffer for next turn

              console.log(`ğŸ¤ Processing ${chunksToProcess.length} audio chunks`);
              await processAudio(callSid, streamSid, chunksToProcess, ws);
              // Note: processAudio will reset isProcessing flag when done
            } else if (currentCallData.audioBuffer.length < MIN_AUDIO_CHUNKS) {
              console.log(`â­ï¸  Skipping - only ${currentCallData.audioBuffer.length} chunks (need ${MIN_AUDIO_CHUNKS})`);
              currentCallData.audioBuffer = [];
            } else if (currentCallData.isProcessing) {
              console.log(`â¸ï¸  Already processing, buffering ${currentCallData.audioBuffer.length} chunks`);
            }
          }, SILENCE_TIMEOUT);
          break;

        case 'stop':
          console.log(`ğŸ“ Call ended: ${callSid}`);
          const endCallData = activeCalls.get(callSid);
          if (endCallData) {
            // × ×§×” ××ª ×›×œ ×”-timeouts
            if (endCallData.idleTimeout) clearTimeout(endCallData.idleTimeout);

            // ğŸ“Š ×œ×•×’ ×¡×˜×˜×™×¡×˜×™×§×•×ª ×©×™×—×”
            const callDuration = Date.now() - endCallData.startTime;
            console.log(`ğŸ“Š Call Stats:`);
            console.log(`   â±ï¸  Duration: ${Math.round(callDuration / 1000)}s`);
            console.log(`   ğŸ”¢ Turns: ${endCallData.turnCount}`);
            console.log(`   ğŸ“š Messages: ${endCallData.conversationHistory.length}`);
          }
          activeCalls.delete(callSid);
          clearTimeout(silenceTimeout);
          break;
      }
    } catch (error) {
      console.error('âŒ Error processing message:', error);
    }
  });

  ws.on('close', () => {
    console.log('ğŸ“ WebSocket connection closed');
    if (callSid) {
      const closeCallData = activeCalls.get(callSid);
      if (closeCallData && closeCallData.idleTimeout) {
        clearTimeout(closeCallData.idleTimeout);
      }
      activeCalls.delete(callSid);
    }
    clearTimeout(silenceTimeout);
  });
});

// ğŸµ ×¤×•× ×§×¦×™×” ×œ×”××™×¨ MP3 ×œ-mulaw ×‘×××¦×¢×•×ª ffmpeg
async function convertMp3ToMulaw(mp3Base64) {
  const tempDir = '/tmp';
  const timestamp = Date.now();
  const mp3Path = path.join(tempDir, `audio_${timestamp}.mp3`);
  const mulawPath = path.join(tempDir, `audio_${timestamp}.ulaw`);

  try {
    // ×›×ª×•×‘ MP3 ×œ×§×•×‘×¥ ×–×× ×™
    const mp3Buffer = Buffer.from(mp3Base64, 'base64');
    await fs.writeFile(mp3Path, mp3Buffer);

    // ×”××¨ ×‘×××¦×¢×•×ª ffmpeg
    await new Promise((resolve, reject) => {
      const ffmpeg = spawn('ffmpeg', [
        '-i', mp3Path,
        '-ar', '8000',      // 8kHz sample rate (Twilio requirement)
        '-ac', '1',         // mono
        '-f', 'mulaw',      // output format
        mulawPath,
        '-y'                // overwrite output file
      ]);

      let stderr = '';
      ffmpeg.stderr.on('data', (data) => {
        stderr += data.toString();
      });

      ffmpeg.on('close', (code) => {
        if (code === 0) {
          resolve();
        } else {
          console.error('âŒ ffmpeg error:', stderr);
          reject(new Error(`ffmpeg exited with code ${code}`));
        }
      });

      ffmpeg.on('error', (err) => {
        reject(err);
      });
    });

    // ×§×¨× ××ª ×§×•×‘×¥ ×”-mulaw
    const mulawBuffer = await fs.readFile(mulawPath);
    const mulawBase64 = mulawBuffer.toString('base64');
    
    console.log('âœ… Converted:', {
      mp3: mp3Buffer.length,
      mulaw: mulawBuffer.length
    });

    // × ×§×” ×§×‘×¦×™× ×–×× ×™×™×
    await fs.unlink(mp3Path).catch(() => {});
    await fs.unlink(mulawPath).catch(() => {});

    return mulawBase64;
  } catch (error) {
    console.error('âŒ Conversion error:', error.message);
    
    await fs.unlink(mp3Path).catch(() => {});
    await fs.unlink(mulawPath).catch(() => {});
    
    throw error;
  }
}

// ğŸ“š ×¤×•× ×§×¦×™×” ×œ× ×™×”×•×œ ×’×•×“×œ ×”×™×¡×˜×•×¨×™×” (×œ×× ×•×¢ ×‘×¢×™×•×ª ×–×™×›×¨×•×Ÿ)
function manageHistorySize(callData) {
  if (callData.conversationHistory.length > MAX_HISTORY_MESSAGES) {
    // ×©××•×¨ ×¨×§ ××ª ×”×”×•×“×¢×•×ª ×”××—×¨×•× ×•×ª, ××‘×œ ×ª××™×“ ×©××•×¨ ××ª ×”×•×“×¢×ª ×”×¤×ª×™×—×”
    const welcomeMessage = callData.conversationHistory.find(msg => msg.type === 'welcome');
    const recentMessages = callData.conversationHistory.slice(-MAX_HISTORY_MESSAGES + 1);

    if (welcomeMessage && !recentMessages.some(msg => msg.type === 'welcome')) {
      callData.conversationHistory = [welcomeMessage, ...recentMessages];
    } else {
      callData.conversationHistory = recentMessages;
    }

    console.log(`ğŸ“š History trimmed to ${callData.conversationHistory.length} messages`);
  }
}

// ğŸš¨ ×¤×•× ×§×¦×™×” ×œ×©×œ×™×—×ª ×”×•×“×¢×ª ×©×’×™××”
async function sendErrorMessage(callSid, streamSid, ws, errorType = 'general') {
  try {
    console.log(`âš ï¸  Sending error recovery message for ${callSid}`);

    const errorMessages = {
      general: '×¡×œ×™×—×”, × ×ª×§×œ×ª×™ ×‘×‘×¢×™×” ×˜×›× ×™×ª. ××¤×©×¨ ×œ×—×–×•×¨ ×¢×œ ××” ×©×××¨×ª?',
      timeout: '×¡×œ×™×—×”, ×œ× ×©××¢×ª×™ ××•×ª×š. ××ª×” ×¢×“×™×™×Ÿ ××™×ª×™?',
      n8n_error: '×¡×œ×™×—×”, ×™×© ×œ×™ ×‘×¢×™×” ×–×× ×™×ª. × ×¡×” ×©×•×‘ ×‘×‘×§×©×”.'
    };

    const payload = {
      callSid,
      streamSid,
      audioData: 'AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA',
      errorRecovery: true,
      errorType,
      errorMessage: errorMessages[errorType] || errorMessages.general
    };

    const response = await axios.post(N8N_WEBHOOK_URL, payload, {
      timeout: 15000,
      headers: { 'Content-Type': 'application/json' }
    });

    if (response.data.success && response.data.audio) {
      let audioPayload = response.data.audio;

      if (response.data.format === 'mp3') {
        audioPayload = await convertMp3ToMulaw(audioPayload);
      }

      await sendAudioToTwilio(ws, streamSid, audioPayload);
      console.log('âœ… Error recovery message sent');
    }
  } catch (error) {
    console.error('âŒ Failed to send error message:', error.message);
    // ×× ×’× ×”×•×“×¢×ª ×”×©×’×™××” × ×›×©×œ×”, ×¤×©×•×˜ × ××©×™×š
  }
}

// â±ï¸  ×¤×•× ×§×¦×™×” ×œ× ×™×”×•×œ timeouts
function setupIdleTimeout(callSid) {
  const callData = activeCalls.get(callSid);
  if (!callData) return;

  // × ×§×” timeout ×§×•×“× ×× ×§×™×™×
  if (callData.idleTimeout) {
    clearTimeout(callData.idleTimeout);
  }

  callData.idleTimeout = setTimeout(async () => {
    const timeSinceLastActivity = Date.now() - callData.lastActivityTime;

    if (timeSinceLastActivity >= MAX_IDLE_TIME && callData.idleWarningsSent < 2) {
      console.log(`âš ï¸  User idle for ${timeSinceLastActivity}ms on call ${callSid}`);
      callData.idleWarningsSent++;
      await sendErrorMessage(callSid, callData.streamSid, callData.ws, 'timeout');

      // ×”×’×“×¨ timeout × ×•×¡×£
      setupIdleTimeout(callSid);
    } else if (callData.idleWarningsSent >= 2) {
      console.log(`ğŸ“ Ending call ${callSid} due to inactivity`);
      callData.ws.close();
    }
  }, MAX_IDLE_TIME);
}

// ×¤×•× ×§×¦×™×” ×œ×©×œ×™×—×ª ×”×•×“×¢×ª ×¤×ª×™×—×”
async function sendWelcomeMessage(callSid, streamSid, ws) {
  try {
    console.log('ğŸ’¬ Generating Hebrew welcome message via n8n');
    
    const silenceBase64 = 'AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA';
    
    const payload = {
      callSid,
      streamSid,
      audioData: silenceBase64,
      welcomeMessage: true
    };
    
    const response = await axios.post(N8N_WEBHOOK_URL, payload, {
      timeout: 30000,
      headers: {
        'Content-Type': 'application/json'
      }
    });

    if (response.data.success && response.data.audio) {
      let audioPayload = response.data.audio;

      // ×”××¨ MP3 ×œ-mulaw
      if (response.data.format === 'mp3') {
        console.log('ğŸ”„ Converting welcome MP3 to mulaw...');
        audioPayload = await convertMp3ToMulaw(audioPayload);
      }

      console.log('ğŸ”Š Playing welcome message');
      await sendAudioToTwilio(ws, streamSid, audioPayload);

      // ğŸ“š ×”×•×¡×£ ×œ×”×™×¡×˜×•×¨×™×™×ª ×”×©×™×—×”
      const callData = activeCalls.get(callSid);
      if (callData && response.data.text) {
        callData.conversationHistory.push({
          role: 'assistant',
          content: response.data.text,
          timestamp: Date.now(),
          type: 'welcome'
        });
        callData.turnCount++;
      }

      console.log('âœ… Welcome message complete');
    }
  } catch (error) {
    console.error('âŒ Error sending welcome:', error.message);
  }
}

async function processAudio(callSid, streamSid, audioChunks, ws) {
  const startTime = Date.now();
  const callData = activeCalls.get(callSid);

  if (!callData) {
    console.error(`âŒ No call data found for ${callSid}`);
    return;
  }

  try {
    const audioBase64 = audioChunks.join('');

    // ğŸ‘‚ ×¢×“×›×Ÿ ×–××Ÿ ×¤×¢×™×œ×•×ª ××—×¨×•×Ÿ
    callData.lastActivityTime = Date.now();

    // ğŸ“Š ×œ×•×’ ××¤×•×¨×˜ ×©×œ ××¦×‘ ×”×©×™×—×”
    const callDuration = Date.now() - callData.startTime;
    console.log(`ğŸ¤ Processing audio for ${callSid}`);
    console.log(`   ğŸ“¦ Chunks: ${audioChunks.length}`);
    console.log(`   ğŸ”¢ Turn: ${callData.turnCount + 1}`);
    console.log(`   â±ï¸  Duration: ${Math.round(callDuration / 1000)}s`);
    console.log(`   ğŸ“š History: ${callData.conversationHistory.length} messages`);

    // ğŸ”„ ×”×›×Ÿ payload ×¢× ×”×™×¡×˜×•×¨×™×” ××œ××”
    const payload = {
      callSid,
      streamSid,
      audioData: audioBase64,
      conversationHistory: callData.conversationHistory, // ğŸ“š CRITICAL: ×©×œ×— ××ª ×›×œ ×”×”×™×¡×˜×•×¨×™×”!
      metadata: {
        turnCount: callData.turnCount,
        callDuration: callDuration,
        timestamp: Date.now()
      }
    };

    const response = await axios.post(N8N_WEBHOOK_URL, payload, {
      timeout: 30000,
      headers: { 'Content-Type': 'application/json' }
    });

    const n8nTime = Date.now() - startTime;
    console.log(`ğŸ“¥ n8n responded in ${n8nTime}ms`);

    if (response.data.success && response.data.audio) {
      let audioPayload = response.data.audio;

      // ×”××¨ MP3 ×œ-mulaw
      if (response.data.format === 'mp3') {
        const convertStart = Date.now();
        console.log('ğŸ”„ Converting response MP3 to mulaw...');
        audioPayload = await convertMp3ToMulaw(audioPayload);
        console.log(`âœ… Converted in ${Date.now() - convertStart}ms`);
      }

      const totalTime = Date.now() - startTime;
      console.log(`ğŸ”Š Sending response (total: ${totalTime}ms)`);

      // âœ… Mark that AI is speaking
      callData.currentAudioPlaying = true;

      try {
        await sendAudioToTwilio(ws, streamSid, audioPayload);
      } catch (audioError) {
        console.error('âŒ Error sending audio to Twilio:', audioError.message);
        // Continue anyway - we'll reset flags below
      }

      // ğŸ“š ×¢×“×›×Ÿ ×”×™×¡×˜×•×¨×™×™×ª ×©×™×—×” ×¢× ×©× ×™ ×”×¦×“×“×™×
      if (response.data.userText) {
        callData.conversationHistory.push({
          role: 'user',
          content: response.data.userText,
          timestamp: Date.now(),
          audioChunks: audioChunks.length
        });
      }

      if (response.data.text) {
        callData.conversationHistory.push({
          role: 'assistant',
          content: response.data.text,
          timestamp: Date.now(),
          processingTime: totalTime
        });
      }

      // ğŸ“š × ×”×œ ×’×•×“×œ ×”×™×¡×˜×•×¨×™×”
      manageHistorySize(callData);

      // ğŸ”¢ ×¢×“×›×Ÿ ××•× ×” ×ª×•×¨×•×ª
      callData.turnCount++;

      // â±ï¸  ××™×¤×•×¡ timeout - ×™×© ×¤×¢×™×œ×•×ª
      setupIdleTimeout(callSid);

      // âœ… Mark that AI finished speaking - ready for next user input!
      callData.currentAudioPlaying = false;
      callData.isProcessing = false; // âœ… Critical: ready to process next input

      console.log(`âœ… Complete response cycle: ${Date.now() - startTime}ms`);
      console.log(`   ğŸ“š History now: ${callData.conversationHistory.length} messages`);
      console.log('ğŸ‘‚ Listening for next user input...');
    } else {
      console.error('âš ï¸  Invalid response from n8n');

      // Reset flags so we can process next input
      callData.currentAudioPlaying = false;
      callData.isProcessing = false;

      await sendErrorMessage(callSid, streamSid, ws, 'n8n_error');
    }
  } catch (error) {
    console.error('âŒ Error processing audio:', error.message);
    if (error.response) {
      console.error('   Status:', error.response.status);
      console.error('   Data:', JSON.stringify(error.response.data, null, 2));
    }

    // ğŸš¨ ×©×œ×— ×”×•×“×¢×ª ×©×’×™××” ×œ××©×ª××©
    try {
      await sendErrorMessage(callSid, streamSid, ws, 'general');
    } catch (recoveryError) {
      console.error('âŒ Failed to recover from error:', recoveryError.message);
    }

    // âœ… Make sure we can process again even if there was an error
    if (callData) {
      callData.currentAudioPlaying = false;
      callData.isProcessing = false;

      // ğŸ“š ×¨×©×•× ×©×’×™××” ×‘×”×™×¡×˜×•×¨×™×”
      callData.conversationHistory.push({
        role: 'system',
        content: 'Error occurred during processing',
        timestamp: Date.now(),
        error: error.message
      });
    }
  }
}

// Helper function to send audio to Twilio
async function sendAudioToTwilio(ws, streamSid, audioBase64) {
  const chunks = Math.ceil(audioBase64.length / CHUNK_SIZE);
  console.log(`ğŸ“¤ Sending ${chunks} audio chunks to Twilio`);
  
  for (let i = 0; i < audioBase64.length; i += CHUNK_SIZE) {
    const chunk = audioBase64.substr(i, CHUNK_SIZE);
    
    ws.send(JSON.stringify({
      event: 'media',
      streamSid: streamSid,
      media: {
        payload: chunk
      }
    }));
    
    // Small delay between chunks to avoid overwhelming Twilio
    if (i % (CHUNK_SIZE * 10) === 0) {
      await new Promise(resolve => setTimeout(resolve, 10));
    }
  }
}
